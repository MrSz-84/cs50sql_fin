{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools.conf\n",
    "import psycopg\n",
    "from psycopg import sql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_1_field(data:list, table_name:str, field_name: str)-> None:\n",
    "    \"\"\"\n",
    "    Skeleton function for adding data to single column tables.\n",
    "\n",
    "    :param data: List of strings or integers representing table contents\n",
    "    :param table_name: String reprexsenting name of the table into which data is going to be added\n",
    "    :raise psycopg.DataError: If data type does not match table restrictions\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    query = sql.SQL('INSERT INTO {table} ({field}) VALUES (%s)')\n",
    "\n",
    "    for elem in data:\n",
    "        cur.execute(\n",
    "            query.format(\n",
    "                table=sql.Identifier(f'{table_name}'),\n",
    "                field=sql.Identifier(f'{field_name}')), (elem,)\n",
    "        )\n",
    "    conn.commit()\n",
    "\n",
    "def iter_over_inputs(data_set:list[dict[list,str,str]])-> None:\n",
    "    \"\"\"\n",
    "    Main loop for iteration over one column tables.\n",
    "\n",
    "    :param data_set: List containing dicts with data, table name and field/column name.\n",
    "    List contains strings or integers representing the data to be added into selected tables.\n",
    "    :param table_name: String representing name of the table into which data is going to be added\n",
    "    :raise KeyError: If key name does not match the pattern\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    for elem in data_set:\n",
    "        data = elem['data']\n",
    "        table = elem['table']\n",
    "        field = elem['field']\n",
    "        new_data, data = check_for_data_1_field(data, table, field)\n",
    "        if new_data:\n",
    "            add_1_field(data, table, field)\n",
    "\n",
    "def check_for_data_1_field(data_:list, table_name:str, field_name:str)-> tuple[bool,list[str]]:\n",
    "    \"\"\"\n",
    "    Skeleton function for checking if there is data inside each of one column tables.\n",
    "    Ads data if there are any new entries, skips if no new data was found. \n",
    "    If DB is empty returns immediately.\n",
    "\n",
    "    :param data_: List containing data to be checked and added. Data is of str or int types.\n",
    "    :param table_name: String representing name of the table into which data is going to be added\n",
    "    :param field_name: String representing name of the field/ column name\n",
    "    :raise psycopg.DataError: If data type does not match table restrictions\n",
    "    :return: A tuple containing bool for logic purposes, anbd the data set to be added\n",
    "    :rtype: tuple[bool, list[str/int]]\n",
    "    \"\"\"\n",
    "    \n",
    "    query = sql.SQL('SELECT {field} FROM {table}')\n",
    "    cur.execute(\n",
    "        query.format(\n",
    "            table=sql.Identifier(table_name),\n",
    "            field=sql.Identifier(field_name))\n",
    "    )\n",
    "    \n",
    "    in_db = pd.DataFrame([elem[0] for elem in cur.fetchall()])\n",
    "    in_db.rename(columns={0: field_name}, inplace=True)\n",
    "    \n",
    "    if len(in_db) == 0:\n",
    "        return (True, data_)\n",
    "    else:\n",
    "        if field_name == 'date':\n",
    "            in_db['date'] = pd.to_datetime(in_db['date'])\n",
    "            in_db = list(in_db['date'])\n",
    "        else: \n",
    "            in_db = list(in_db[field_name])\n",
    "        \n",
    "        if len(in_db) != 0 and table_name in avoid_adding:\n",
    "            print(f'>>> Not adding to {table_name}. No new data found.')\n",
    "            return (False, list(''))\n",
    "        in_df = pd.DataFrame(data_)\n",
    "        in_df = in_df.rename(columns={0: field_name})\n",
    "        if field_name == 'date':\n",
    "            in_df['date'] = pd.to_datetime(in_df['date'])\n",
    "        \n",
    "        # we check if df contains new data in comparison to DB\n",
    "        new_data = in_df[~in_df.isin(in_db)].dropna()\n",
    "        new_data = new_data[field_name]\n",
    "        new_data = list(new_data)\n",
    "        \n",
    "        if len(new_data) != 0:\n",
    "            print(f'>>> Adding to {table_name}. New data found.')\n",
    "            return (True, new_data)\n",
    "        else:\n",
    "            return (False, list(''))\n",
    "\n",
    "def add_3_fields(data_set:dict[pd.DataFrame,str,list])-> None:\n",
    "    \"\"\"\n",
    "    Function adding data into mediums table, which consists of 3 columns.\n",
    "\n",
    "    :param data_set: A dict contaning data to be added, table name, and field / column name.\n",
    "    Data is a Pandas DataFrame, table name and field name are both strings.\n",
    "    :raise KeyError: If key name does not match the pattern\n",
    "    :raise psycopg.DataError: If table or field names doesn't match those in the DB\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    col = data_set['data'].columns.values.tolist()\n",
    "    \n",
    "    query = sql.SQL('INSERT INTO {table} ({field1}, {field2}, {field3}) VALUES (%s, %s ,%s)')\n",
    "\n",
    "    for _, elem in data_set['data'].iterrows():\n",
    "        cur.execute(\n",
    "            query.format(\n",
    "                table=sql.Identifier(data_set['table']),\n",
    "                field1=sql.Identifier(data_set['fields'][0]),\n",
    "                field2=sql.Identifier(data_set['fields'][1]),\n",
    "                field3=sql.Identifier(data_set['fields'][2])), \n",
    "                (elem[col[0]], elem[col[1]], elem[col[2]],)\n",
    "        )\n",
    "        \n",
    "    conn.commit()\n",
    "    \n",
    "def check_for_data_3_fields(fields:list[str], table_name: str, submediums: pd.DataFrame)-> tuple[bool,pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns a bool for logic purposes and data to be added into mediums table.\n",
    "    If DB is empty returns original DF. During data update process returns the data not present in the DB\n",
    "    or indicates there is nothing to be added.\n",
    "\n",
    "    :param fields: A list containing field / column names represented as a str\n",
    "    :param table_name: Name of the table into which data is going to be added as a str\n",
    "    :param submediums: Pandas DataFrame containing data to add.\n",
    "    :raise psycopg.DataError: If data type does not match table restrictions\n",
    "    :return: Tuple containing bool for logic purposes and a Pandas DataFrame \n",
    "    as data to be added into the DB during the update\n",
    "    :rtype: tuple[bool, pd.DataFrame]\n",
    "    \"\"\"\n",
    "    \n",
    "    query = sql.SQL('SELECT id, {field1}, {field2}, {field3} FROM {table}')\n",
    "    cur.execute(\n",
    "        query.format(\n",
    "            table=sql.Identifier(table_name),\n",
    "            field1=sql.Identifier(fields[0]),\n",
    "            field2=sql.Identifier(fields[1]),\n",
    "            field3=sql.Identifier(fields[2]))\n",
    "    )\n",
    "    \n",
    "    in_db = pd.DataFrame(cur.fetchall())\n",
    "    in_db.rename(columns={0: 'id',1: fields[0], 2: fields[1], 3: fields[2]}, inplace=True)\n",
    "    \n",
    "    if len(in_db) == 0:\n",
    "        return (True, submediums)\n",
    "    else:\n",
    "        in_db = list(in_db[fields[0]])\n",
    "        in_df = submediums.copy()\n",
    "        # we check if df contains new data in comparison to DB\n",
    "        new_data = in_df[~in_df.isin(in_db)].dropna()\n",
    "        \n",
    "        if len(new_data) != 0 :\n",
    "            print(f'>>> Adding to {table_name}. New data found.')\n",
    "            return (True, new_data)\n",
    "        print(f'>>> Not adding to {table_name}. No new data found.')\n",
    "        return (False, submediums)\n",
    "\n",
    "def add_8_fields(data_set:dict[pd.DataFrame,str,list[str]])-> None:\n",
    "    \"\"\"\n",
    "    Function adding data into ad_time_details table, which consists of 8 columns.\n",
    "\n",
    "    :param data_set: A dict contaning data to be added, table name, and field / column name.\n",
    "    Data is a Pandas DataFrame, table name and field name are both strings.\n",
    "    :raise KeyError: If key name does not match the pattern\n",
    "    :raise psycopg.DataError: If table or field names doesn't match those in the DB\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    col = data_set['data'].columns.values.tolist()\n",
    "    \n",
    "    query = sql.SQL(\n",
    "        '''\n",
    "        INSERT INTO {table} ({field1}, {field2}, {field3}, {field4}, {field5}, {field6}, {field7}, {field8}) \n",
    "        VALUES (%s, %s ,%s, %s ,%s, %s ,%s ,%s)\n",
    "        ''')\n",
    "\n",
    "    for _, elem in data_set['data'].iterrows():\n",
    "        cur.execute(\n",
    "            query.format(\n",
    "                table=sql.Identifier(data_set['table']),\n",
    "                field1=sql.Identifier(data_set['fields'][0]),\n",
    "                field2=sql.Identifier(data_set['fields'][1]),\n",
    "                field3=sql.Identifier(data_set['fields'][2]),\n",
    "                field4=sql.Identifier(data_set['fields'][3]),\n",
    "                field5=sql.Identifier(data_set['fields'][4]),\n",
    "                field6=sql.Identifier(data_set['fields'][5]),\n",
    "                field7=sql.Identifier(data_set['fields'][6]),\n",
    "                field8=sql.Identifier(data_set['fields'][7])), \n",
    "                (elem[col[0]], elem[col[1]], elem[col[2]], \n",
    "                 elem[col[3]], elem[col[4]], elem[col[5]], \n",
    "                 elem[col[6]], elem[col[7]],\n",
    "                )\n",
    "        )\n",
    "        \n",
    "    conn.commit()\n",
    "\n",
    "def add_10_fields(data_set:dict[pd.DataFrame,str,list[str]])-> None:\n",
    "    \"\"\"\n",
    "    Function adding data into ad_time_details table, which consists of 10 columns.\n",
    "\n",
    "    :param data_set: A dict contaning data to be added, table name, and field / column name.\n",
    "    Data is a Pandas DataFrame, table name and field name are both strings.\n",
    "    :raise KeyError: If key name does not match the pattern\n",
    "    :raise psycopg.DataError: If table or field names doesn't match those in the DB\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    col = data_set['data'].columns.values.tolist()\n",
    "    \n",
    "    query = sql.SQL(\n",
    "        '''\n",
    "        INSERT INTO {table} ({field1}, {field2}, {field3}, {field4}, {field5}, {field6}, {field7}, {field8}, {field9}, {field10}) \n",
    "        VALUES (%s, %s ,%s, %s ,%s, %s ,%s ,%s ,%s ,%s)\n",
    "        ''')\n",
    "\n",
    "    for _, elem in data_set['data'].iterrows():\n",
    "        cur.execute(\n",
    "            query.format(\n",
    "                table=sql.Identifier(data_set['table']),\n",
    "                field1=sql.Identifier(data_set['fields'][0]),\n",
    "                field2=sql.Identifier(data_set['fields'][1]),\n",
    "                field3=sql.Identifier(data_set['fields'][2]),\n",
    "                field4=sql.Identifier(data_set['fields'][3]),\n",
    "                field5=sql.Identifier(data_set['fields'][4]),\n",
    "                field6=sql.Identifier(data_set['fields'][5]),\n",
    "                field7=sql.Identifier(data_set['fields'][6]),\n",
    "                field8=sql.Identifier(data_set['fields'][7]),\n",
    "                field9=sql.Identifier(data_set['fields'][8]),\n",
    "                field10=sql.Identifier(data_set['fields'][9])), \n",
    "                (elem[col[0]], elem[col[1]], elem[col[2]], \n",
    "                 elem[col[3]], elem[col[4]], elem[col[5]], \n",
    "                 elem[col[6]], elem[col[7]], elem[col[8]], \n",
    "                 elem[col[9]],\n",
    "                )\n",
    "        )\n",
    "        \n",
    "    conn.commit()\n",
    "\n",
    "def get_id_for_submediums(fields:list[str], table_:str)-> tuple[bool, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Gets IDs from reference tables to mediums table. \n",
    "    Mainly connects submediums with broadcaster and reach tables.\n",
    "    Returns a bool for logic purposes and data to be added into mediums.\n",
    "\n",
    "    :param fields: A list containing field / column names represented as a str\n",
    "    :param table_: Name of the table out of which the data is going to be pulled, \n",
    "    represented as a str\n",
    "    :raise psycopg.DataError: If data type does not match table restrictions\n",
    "    :return: Tuple containing bool for logic purposes and a Pandas DataFrame \n",
    "    as data to be added into the DB during the update or initial DB fill.\n",
    "    :rtype: tuple[bool, pd.DataFrame]\n",
    "    \"\"\"\n",
    "    \n",
    "    submediums = df[['submedium', 'wydawca_nadawca', 'zasięg medium']].sort_values(by='submedium')\n",
    "    submediums.drop_duplicates(subset=['submedium'], keep='first', inplace=True, ignore_index=True)\n",
    "    \n",
    "    if sum(submediums.value_counts()) != submediums.index.max() + 1:\n",
    "        exit('Max index different than the length of the list.')\n",
    "    \n",
    "    query1 = sql.SQL('SELECT {fields} FROM {table}').format(\n",
    "    fields=sql.SQL(',').join([\n",
    "        sql.Identifier('broadcaster'),\n",
    "        sql.Identifier('id')\n",
    "    ]),\n",
    "    table=sql.Identifier('broadcasters'))\n",
    "    cur.execute(query1)\n",
    "    broadcasters = dict(cur.fetchall())\n",
    "\n",
    "    query2 = sql.SQL('SELECT {fields} FROM {table}').format(\n",
    "        fields=sql.SQL(',').join([\n",
    "            sql.Identifier('reach'),\n",
    "            sql.Identifier('id')\n",
    "        ]),\n",
    "        table=sql.Identifier('ad_reach'))\n",
    "    cur.execute(query2)\n",
    "    ad_reach = dict(cur.fetchall())\n",
    "\n",
    "    submediums['wydawca_nadawca'] = submediums['wydawca_nadawca'].map(broadcasters)\n",
    "    submediums['zasięg medium'] = submediums['zasięg medium'].map(ad_reach)\n",
    "    \n",
    "    trigger, submediums = check_for_data_3_fields(fields, table_, submediums)\n",
    "    \n",
    "    return (trigger, submediums)\n",
    "\n",
    "def get_id_for_ad_time(fields: list[str], table_: str)-> tuple[bool,pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Gets IDs from reference tables to ad_time_details table. \n",
    "    Mainly connects time details of singular ad emission with other tables containing details via IDs.\n",
    "    This function populates one of two core tables in this DB.\n",
    "    Returns a bool for logic purposes and data to be added into mediums.\n",
    "\n",
    "    :param fields: A list containing field / column names represented as a str\n",
    "    :param table_: Name of the table out of which the data is going to be pulled, \n",
    "    represented as a str\n",
    "    :raise psycopg.DataError: If data type does not match table restrictions\n",
    "    :return: Tuple containing bool for logic purposes and a Pandas DataFrame \n",
    "    as data to be added into the DB during the update or initial DB fill.\n",
    "    :rtype: tuple[bool, pd.DataFrame]\n",
    "    \"\"\"\n",
    "    \n",
    "    ad_time = df[['data', 'godzina_bloku_reklamowego', 'gg', 'mm', 'dl_mod', 'daypart', 'dł_ujednolicona', 'ad_time_details']]\n",
    "    ad_time.index = ad_time.index + get_index_val(table_)\n",
    "\n",
    "    query1 = sql.SQL('SELECT {fields} FROM {table}').format(\n",
    "        fields=sql.SQL(',').join([\n",
    "            sql.Identifier('length'),\n",
    "            sql.Identifier('id')\n",
    "            ]),\n",
    "        table=sql.Identifier('unified_lengths'))\n",
    "\n",
    "    cur.execute(query1)\n",
    "    unified_lengths = dict(cur.fetchall())\n",
    "\n",
    "    query2 = sql.SQL('SELECT {fields} FROM {table}').format(\n",
    "        fields=sql.SQL(',').join([\n",
    "            sql.Identifier('daypart'),\n",
    "            sql.Identifier('id')\n",
    "            ]),\n",
    "        table=sql.Identifier('dayparts'))\n",
    "\n",
    "    cur.execute(query2)\n",
    "    dayparts = dict(cur.fetchall())\n",
    "\n",
    "    ad_time.loc[:, 'daypart'] = ad_time['daypart'].map(dayparts)\n",
    "    ad_time.loc[:, 'dł_ujednolicona'] = ad_time['dł_ujednolicona'].map(unified_lengths)\n",
    "    \n",
    "    trigger, ad_time = get_min_max_date(fields, table_, ad_time)\n",
    "    \n",
    "    return (trigger, ad_time)\n",
    "\n",
    "def get_id_for_ads_desc(fields: list[str], table_: str)-> tuple[bool,pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Gets IDs from reference tables to ads_desc table. \n",
    "    Mainly connects other tables and data of singular ad emission via IDs with other tables.\n",
    "    This function populates one of two core tables in this DB.\n",
    "    Returns a bool for logic purposes and data to be added into mediums.\n",
    "\n",
    "    :param fields: A list containing field / column names represented as a str\n",
    "    :param table_: Name of the table out of which the data is going to be pulled, \n",
    "    represented as a str\n",
    "    :raise psycopg.DataError: If data type does not match table restrictions\n",
    "    :return: Tuple containing bool for logic purposes and a Pandas DataFrame \n",
    "    as data to be added into the DB during the update or initial DB fill.\n",
    "    :rtype: tuple[bool, pd.DataFrame]\n",
    "    \"\"\"\n",
    "    \n",
    "    ads_desc = df[['data', 'opis_reklamy', 'kod_reklamy', 'brand', 'submedium', 'ad_time_details', 'produkt(4)', 'koszt', 'l_emisji', 'typ_reklamy']]\n",
    "    ads_desc.index = ads_desc.index + get_index_val(table_)\n",
    "\n",
    "    query1 = sql.SQL('SELECT {fields} FROM {table}').format(\n",
    "        fields=sql.SQL(',').join([\n",
    "            sql.Identifier('brand'),\n",
    "            sql.Identifier('id')\n",
    "            ]),\n",
    "        table=sql.Identifier('brands'))\n",
    "\n",
    "    cur.execute(query1)\n",
    "    brands_id = dict(cur.fetchall())\n",
    "\n",
    "\n",
    "    query2 = sql.SQL('SELECT {fields} FROM {table}').format(\n",
    "        fields=sql.SQL(',').join([\n",
    "            sql.Identifier('submedium'),\n",
    "            sql.Identifier('id')\n",
    "            ]),\n",
    "        table=sql.Identifier('mediums'))\n",
    "\n",
    "    cur.execute(query2)\n",
    "    medium_id = dict(cur.fetchall())\n",
    "\n",
    "\n",
    "    query3 = sql.SQL('SELECT {fields} FROM {table}').format(\n",
    "        fields=sql.SQL(',').join([\n",
    "            sql.Identifier('ad_code'),\n",
    "            sql.Identifier('id')\n",
    "            ]),\n",
    "        table=sql.Identifier('ad_time_details'))\n",
    "\n",
    "    cur.execute(query3)\n",
    "    ad_time_details_id = dict(cur.fetchall())\n",
    "\n",
    "\n",
    "    query4 = sql.SQL('SELECT {fields} FROM {table}').format(\n",
    "        fields=sql.SQL(',').join([\n",
    "            sql.Identifier('product_type'),\n",
    "            sql.Identifier('id')\n",
    "            ]),\n",
    "        table=sql.Identifier('product_types'))\n",
    "\n",
    "    cur.execute(query4)\n",
    "    product_type_id = dict(cur.fetchall())\n",
    "\n",
    "    ads_desc.loc[:, 'brand'] = ads_desc['brand'].map(brands_id)\n",
    "    ads_desc.loc[:, 'submedium'] = ads_desc['submedium'].map(medium_id)\n",
    "    ads_desc.loc[:, 'ad_time_details'] = ads_desc['ad_time_details'].map(ad_time_details_id)\n",
    "    ads_desc.loc[:, 'produkt(4)'] = ads_desc['produkt(4)'].map(product_type_id)\n",
    "    \n",
    "    trigger, ads_desc = get_min_max_date(fields, table_, ads_desc)\n",
    "    \n",
    "    return (trigger, ads_desc)\n",
    "\n",
    "def get_colum_names(table_name:str)->list[str]:\n",
    "    \"\"\"\n",
    "    A function which returns the names of selected table from the DB.\n",
    "\n",
    "    :raise psycopg.DatabaseError: If column names does not match DB contents\n",
    "    :return: List containing all the column names present in selected table. \n",
    "    :rtype: list[str]\n",
    "    \"\"\"\n",
    "    \n",
    "    query = sql.SQL(\n",
    "    '''\n",
    "    SELECT c.column_name \n",
    "    FROM information_schema.columns c \n",
    "    WHERE c.table_name = %s\n",
    "    ORDER BY c.ordinal_position;\n",
    "    ''').format()\n",
    "    cur.execute(query, (table_name,))\n",
    "    table_data = cur.fetchall()\n",
    "    temp = []\n",
    "    for elem in table_data[1:]:\n",
    "        temp.append(elem[0])\n",
    "    table_data = temp\n",
    "    \n",
    "    return table_data\n",
    "\n",
    "def get_index_val(table_name: str)-> int:\n",
    "    \"\"\"\n",
    "    Function gets max index value from the selected table and returns it as an integer increased by one.\n",
    "    When the table is empty, returns 1\n",
    "\n",
    "    :param table_name: Name of the table out of which the data is going to be pulled, \n",
    "    represented as a str\n",
    "    :raise psycopg.DataError: If data type does not match table restrictions\n",
    "    :return: number representiung max index value of selected table icreased by 1\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    \n",
    "    query = sql.SQL('SELECT MAX(id) FROM {table};')\n",
    "    cur.execute(query.format(table=sql.Identifier(table_name)))\n",
    "    ind = cur.fetchone()\n",
    "    if ind[0] == None:\n",
    "        return 1\n",
    "    else:\n",
    "        return ind[0] + 1\n",
    "\n",
    "def get_min_max_date(fields: list[str], table_: str, dataframe: pd.DataFrame)-> tuple[bool, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Gets max and min dates from selected table. Then checks if dates present in the DF passed as a parameter\n",
    "    are outside of dates range. If so, allows data insertion into the DB, if not it informs the user, \n",
    "    and proceedes with the rest of the code.\n",
    "\n",
    "    :param fields: A list containing field / column names represented as a str\n",
    "    :param table_: Name of the table out of which the data is going to be pulled, \n",
    "    represented as a str\n",
    "    :param dataframe: Pandas DataFrame with the new data to be checked if not present in selected table\n",
    "    :raise psycopg.DatabaseError: If column names does not match DB contents\n",
    "    :return: Tuple containing bool for logic purposes and a Pandas DataFrame \n",
    "    as data to be added into the DB during the update or initial DB fill.\n",
    "    :rtype: tuple[bool, pd.DataFrame]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get max date from DB\n",
    "    query = sql.SQL('SELECT MAX({field1}) FROM {table};')\n",
    "    cur.execute(\n",
    "        query.format(\n",
    "            table=sql.Identifier(table_),\n",
    "            field1=sql.Identifier(fields[0])\n",
    "        )\n",
    "    )\n",
    "    in_db_max = pd.Timestamp(cur.fetchone()[0])\n",
    "    \n",
    "    # Get min date from DB\n",
    "    query = sql.SQL('SELECT MIN({field1}) FROM {table};')\n",
    "    cur.execute(\n",
    "        query.format(\n",
    "            table=sql.Identifier(table_),\n",
    "            field1=sql.Identifier(fields[0])\n",
    "        )\n",
    "    )\n",
    "    in_db_min = pd.Timestamp(cur.fetchone()[0])\n",
    "    \n",
    "    # Get max and min date from DF\n",
    "    in_df_max = dataframe['data'].max()\n",
    "    in_df_min = dataframe['data'].min()\n",
    "    \n",
    "    # Check if min and max dates from DF are between range of dates from DB\n",
    "    min_df_in_db_range = in_db_min <= in_df_min <= in_db_max\n",
    "    max_df_in_db_range = in_db_min <= in_df_max <= in_db_max\n",
    "    \n",
    "    # Main logic add if empty or when dates not present in DB.\n",
    "    if  pd.isnull(in_db_max) or pd.isnull(in_db_min) :\n",
    "        return (True, dataframe)\n",
    "    elif not min_df_in_db_range and not max_df_in_db_range:\n",
    "        print(f'>>> Adding to {table_}. New data found.')\n",
    "        return (True, dataframe)\n",
    "    else:\n",
    "        print(f'>>> Not adding to {table_}. One or more dates already in DB.')\n",
    "        print(f'>>> Check the data you want to insert into DB.')\n",
    "        return (False, dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16196/4265642212.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  df['ad_time_details'] = df[['data', 'kod_reklamy']].apply(lambda x: f'{x[0]} - {x[1]} - {ind[x.name]}', axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Openes connection to the DB\n",
    "print('Oppening connection.')\n",
    "conn = psycopg.connect(\n",
    "    f'''dbname={tools.conf.DB}\n",
    "        user={tools.conf.USER}\n",
    "        host={tools.conf.HOST}\n",
    "        port={tools.conf.PORT}\n",
    "    '''\n",
    ")\n",
    "\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating DataFrame.')\n",
    "# Reads the dataframe\n",
    "df = pd.read_csv('./data/baza2.csv', delimiter=';', thousands=',', dtype={'dł_ujednolicona': 'object'}, encoding='utf-8', parse_dates=['data'])\n",
    "df.sort_values(by='data', axis=0, inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "df.drop('index', axis=1, inplace=True)\n",
    "ind = df.index.values + get_index_val('ads_desc')\n",
    "df['ad_time_details'] = df[['data', 'kod_reklamy']].apply(lambda x: f'{x[\"data\"]} - {x[\"kod_reklamy\"]} - {ind[x.name]}', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets for simple tables\n",
    "dow2 = ['Poniedziałek', 'Wtorek', 'Środa', 'Czwartek', 'Piątek',\n",
    "        'Sobota', 'Niedziela']\n",
    "months = [\n",
    "    'Styczeń', 'Luty', 'Marzec', 'Kwiecień', 'Maj',\n",
    "    'Czerwiec', 'Lipiec', 'Sierpień', 'Wrzesień',\n",
    "    'Październik', 'Listopad', 'Grudzień'\n",
    "]\n",
    "dates = df['data'].unique()\n",
    "brands = df['brand'].sort_values().unique()\n",
    "lengths = df['dł_ujednolicona'].sort_values().unique()\n",
    "dayparts = df['daypart'].unique()\n",
    "product_types = df['produkt(4)'].sort_values().unique()\n",
    "broadcasters = df['wydawca_nadawca'].sort_values().unique()\n",
    "reaches = df['zasięg medium'].unique()\n",
    "\n",
    "data_set = [{'data': dow2, 'table': 'pl_dow_names', 'field': 'dow_name'},\n",
    "            {'data': months, 'table': 'pl_month_names', 'field': 'month_name'},\n",
    "            {'data': dates, 'table': 'date_time', 'field': 'date'},\n",
    "            {'data': brands, 'table': 'brands', 'field': 'brand'},\n",
    "            {'data': lengths, 'table': 'unified_lengths', 'field': 'length'},\n",
    "            {'data': dayparts, 'table': 'dayparts', 'field': 'daypart'},\n",
    "            {'data': product_types, 'table': 'product_types', 'field': 'product_type'},\n",
    "            {'data': broadcasters, 'table': 'broadcasters', 'field': 'broadcaster'},\n",
    "            {'data': reaches, 'table': 'ad_reach', 'field': 'reach'},\n",
    "            ]\n",
    "\n",
    "avoid_adding = ['pl_dow_names', 'pl_month_names', 'dayparts', 'ad_reach']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16196/222699361.py:213: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ads_desc['brand'] = ads_desc['brand'].map(brands_id)\n",
      "/tmp/ipykernel_16196/222699361.py:214: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ads_desc['submedium'] = ads_desc['submedium'].map(medium_id)\n",
      "/tmp/ipykernel_16196/222699361.py:215: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ads_desc['ad_time_details'] = ads_desc['ad_time_details'].map(ad_time_details_id)\n",
      "/tmp/ipykernel_16196/222699361.py:216: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ads_desc['produkt(4)'] = ads_desc['produkt(4)'].map(product_type_id)\n"
     ]
    }
   ],
   "source": [
    "# Inserting data into simple tables\n",
    "print('Inserting data to one input tables.')\n",
    "try:\n",
    "    iter_over_inputs(data_set)\n",
    "except psycopg.OperationalError as e:\n",
    "    conn.close()\n",
    "    print('Failed to input the data.')\n",
    "    print(f'Error: {e}')\n",
    "\n",
    "# Create and insert data into mediums table\n",
    "print('Inserting data to the three input table.')\n",
    "fields = get_colum_names('mediums')\n",
    "trigger, submediums = get_id_for_submediums(fields, 'mediums')\n",
    "data_set2 = {'data': submediums, 'table': 'mediums', 'fields': fields}\n",
    "if trigger:\n",
    "    try:\n",
    "        add_3_fields(data_set2)\n",
    "    except psycopg.OperationalError as e:\n",
    "        conn.close()\n",
    "        print('Failed to input the data.')\n",
    "        print(f'Error: {e}')\n",
    "\n",
    "# Create and insert data into ad_time_details table\n",
    "print('Inserting data to the eight input table.')\n",
    "fields = get_colum_names('ad_time_details')\n",
    "trigger, ad_time = get_id_for_ad_time(fields, 'ad_time_details')\n",
    "data_set3 = {'data': ad_time, 'table': 'ad_time_details', 'fields': fields}\n",
    "if trigger:\n",
    "    try:\n",
    "        add_8_fields(data_set3)\n",
    "    except psycopg.OperationalError as e:\n",
    "        conn.close()\n",
    "        print('Failed to input the data.')\n",
    "        print(f'Error: {e}')\n",
    "\n",
    "# Create and insert data into ad_time_details table\n",
    "print('Inserting data to the ten input table.')\n",
    "fields = get_colum_names('ads_desc')\n",
    "trigger, ads_desc = get_id_for_ads_desc(fields, 'ads_desc')\n",
    "data_set4 = {'data': ads_desc, 'table': 'ads_desc', 'fields': fields}\n",
    "if trigger:\n",
    "    try:\n",
    "        add_10_fields(data_set4)\n",
    "    except psycopg.OperationalError as e:\n",
    "        conn.close()\n",
    "        print('Failed to input the data.')\n",
    "        print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_val(table_name: str, con: sqlite3.Connection, cur: sqlite3.Cursor)-> int:\n",
    "    \"\"\"\n",
    "    Function gets max index value from the selected table and returns it as an integer increased by one.\n",
    "    When the table is empty, returns 1\n",
    "\n",
    "    :param table_name: Name of the table out of which the data is going to be pulled, \n",
    "    represented as a str\n",
    "    :param con: Is a connection object, pointing to a DB\n",
    "    :param cur: Is a cursor object created for con object\n",
    "    :raise psycopg.DataError: If data type does not match table restrictions\n",
    "    :return: number representiung max index value of selected table icreased by 1\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    \n",
    "    query = (f'SELECT MAX(id) FROM {table_name};')\n",
    "    cur.execute(query)\n",
    "    ind = cur.fetchone()\n",
    "    if ind[0] == None:\n",
    "        return 1\n",
    "    else:\n",
    "        return ind[0] + 1\n",
    "    \n",
    "def iter_over_inputs(data_set:list[dict[list,str,str]], con: sqlite3.Connection, \n",
    "                        cur: sqlite3.Cursor, avoid_adding: list[str])-> None:\n",
    "    \"\"\"\n",
    "    Main loop for iteration over one column tables.\n",
    "\n",
    "    :param data_set: List containing dicts with data, table name and field/column name.\n",
    "    List contains strings or integers representing the data to be added into selected tables.\n",
    "    :param table_name: String representing name of the table into which data is going to be added\n",
    "    :param con: Is a connection object, pointing to a DB\n",
    "    :param cur: Is a cursor object created for con object\n",
    "    :param avoid_adding: List of tablet which doesn't need to be updated\n",
    "    :raise KeyError: If key name does not match the pattern\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    for elem in data_set:\n",
    "        data = elem['data']\n",
    "        table = elem['table']\n",
    "        field = elem['field']\n",
    "        new_data, data = check_for_data_1_field(data, table, field, con, cur, avoid_adding)\n",
    "        if new_data:\n",
    "            add_1_field(data, table, field, con, cur)\n",
    "\n",
    "def add_1_field(data:list, table_name:str, field_name: str, \n",
    "                con: sqlite3.Connection, cur: sqlite3.Cursor)-> None:\n",
    "    \"\"\"\n",
    "    Skeleton function for adding data to single column tables.\n",
    "\n",
    "    :param data: List of strings or integers representing table contents\n",
    "    :param table_name: String reprexsenting name of the table into which data is going to be added\n",
    "    :param con: Is a connection object, pointing to a DB\n",
    "    :param cur: Is a cursor object created for con object\n",
    "    :raise sqlite3.ProgrammingError: If there are any error raised by the DB-API\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    query = (f'INSERT INTO {table_name} ({field_name}) VALUES(:name);')\n",
    "    for elem in data:\n",
    "        to_add = {'name': str(elem)}\n",
    "        cur.execute(query, to_add)\n",
    "    con.commit()\n",
    "\n",
    "def check_for_data_1_field(data_:list, table_name:str, field_name:str, con: sqlite3.Connection, \n",
    "                            cur: sqlite3.Cursor, avoid_adding: list[str])-> tuple[bool,list[str]]:\n",
    "    \"\"\"\n",
    "    Skeleton function for checking if there is data inside each of one column tables.\n",
    "    Ads data if there are any new entries, skips if no new data was found. \n",
    "    If DB is empty returns immediately.\n",
    "\n",
    "    :param data_: List containing data to be checked and added. Data is of str or int types.\n",
    "    :param table_name: String representing name of the table into which data is going to be added\n",
    "    :param field_name: String representing name of the field/ column name\n",
    "    :param avoid_adding: List of tablet which doesn't need to be updated\n",
    "    :raise sqlite3.ProgrammingError: If there is an error raised by the DB-API\n",
    "    :return: A tuple containing bool for logic purposes, anbd the data set to be added\n",
    "    :rtype: tuple[bool, list[str/int]]\n",
    "    \"\"\"\n",
    "    \n",
    "    query = (f'SELECT {field_name} FROM {table_name}')\n",
    "    cur.execute(query)\n",
    "    \n",
    "    in_db = pd.DataFrame([elem[0] for elem in cur.fetchall()])\n",
    "    in_db.rename(columns={0: field_name}, inplace=True)\n",
    "    \n",
    "    if len(in_db) == 0:\n",
    "        return (True, data_)\n",
    "    else:\n",
    "        if field_name == 'data':\n",
    "            # in_db['data'] = pd.to_datetime(in_db['data'])\n",
    "            in_db = list(in_db['data'])\n",
    "        else: \n",
    "            in_db = list(in_db[field_name])\n",
    "        \n",
    "        if len(in_db) != 0 and table_name in avoid_adding:\n",
    "            print(f'>>> Not adding to {table_name}. No new data found.')\n",
    "            return (False, list(''))\n",
    "        in_df = pd.DataFrame(data_)\n",
    "        in_df = in_df.rename(columns={0: field_name})\n",
    "        if field_name == 'data':\n",
    "            pass\n",
    "            # in_df['data'] = pd.to_datetime(in_df['data'])\n",
    "        \n",
    "        # we check if df contains new data in comparison to DB\n",
    "        new_data = in_df[~in_df.isin(in_db)].dropna()\n",
    "        new_data = new_data[field_name]\n",
    "        new_data = list(new_data)\n",
    "        \n",
    "        if len(new_data) != 0:\n",
    "            print(f'>>> Adding to {table_name}. New data found.')\n",
    "            return (True, new_data)\n",
    "        else:\n",
    "            print(f'>>> Not adding to {table_name}. No new data found.')\n",
    "            return (False, list(''))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.Connection('./radio_ads.db')\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataFrame.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Integer column has NA values in column 15",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCreating DataFrame.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Reads the dataframe\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/live_4.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDzień\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDzień tygodnia\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNr. tyg.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRok\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMiesiąc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mZasięg medium\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBrand\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mProdukt(4)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mKod Reklamy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mint32\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOpis Reklamy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTyp reklamy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWydawca/Nadawca\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSubmedium\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdł./mod\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mint8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGG\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mint8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mint8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mKoszt [zł]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mInt32\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL.emisji\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mint8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDaypart\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdł. Ujednolicona\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGodzina bloku reklamowego\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m                 \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m df\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m df\u001b[38;5;241m.\u001b[39mreset_index(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/python/3.12.2/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.2/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.2/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/python/3.12.2/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:239\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "File \u001b[0;32mparsers.pyx:820\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:921\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:1066\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:1105\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:1226\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Integer column has NA values in column 15"
     ]
    }
   ],
   "source": [
    "print('Creating DataFrame.')\n",
    "# Reads the dataframe\n",
    "df = pd.read_csv('../data/live_6.csv', delimiter=';', thousands=',',\n",
    "                 dtype={'Dzień': 'category', 'Dzień tygodnia': 'category', \n",
    "                        'Nr. tyg.': 'category', 'Rok': 'category',\n",
    "                        'Miesiąc': 'category', 'Zasięg medium': 'category',\n",
    "                        'Brand': 'category', 'Produkt(4)': 'category',\n",
    "                        'Kod Reklamy': 'int32', 'Opis Reklamy': 'object',\n",
    "                        'Typ reklamy': 'category', 'Wydawca/Nadawca': 'category',\n",
    "                        'Submedium': 'category', 'dł./mod': 'int8',\n",
    "                        'GG': 'int8', 'MM': 'int8', 'Koszt [zł]': 'Int32',\n",
    "                        'L.emisji': 'int8', 'Daypart': 'category',\n",
    "                        'dł. Ujednolicona': 'category', \n",
    "                        'Godzina bloku reklamowego': 'category'}, \n",
    "                 encoding='utf-8', parse_dates=['Data'], low_memory=False\n",
    "                 )\n",
    "\n",
    "df.sort_values(by='Data', axis=0, inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "df.drop('index', axis=1, inplace=True)\n",
    "ind = df.index.values + get_index_val('spoty', con, cur)\n",
    "df['Detale_kod_reklamy'] = df[['Data', 'Kod Reklamy']].apply(lambda x: f'{x[\"Data\"]} - {x[\"Kod Reklamy\"]} - {ind[x.name]}', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets for simple tables\n",
    "dow2 = ['Poniedziałek', 'Wtorek', 'Środa', 'Czwartek', 'Piątek',\n",
    "        'Sobota', 'Niedziela']\n",
    "months = [\n",
    "    'Styczeń', 'Luty', 'Marzec', 'Kwiecień', 'Maj',\n",
    "    'Czerwiec', 'Lipiec', 'Sierpień', 'Wrzesień',\n",
    "    'Październik', 'Listopad', 'Grudzień'\n",
    "]\n",
    "dates = df['Data'].dt.strftime('%Y-%m-%d').unique()\n",
    "brands = df['Brand'].sort_values().unique()\n",
    "lengths = ['10', '15', '20', '30', '45', '60',]\n",
    "dayparts = df['Daypart'].unique()\n",
    "product_types = df['Produkt(4)'].sort_values().unique()\n",
    "broadcasters = df['Wydawca/Nadawca'].sort_values().unique()\n",
    "reaches = df['Zasięg medium'].unique()\n",
    "\n",
    "data_set = [{'data': dow2, 'table': 'dni_tyg', 'field': 'dzien_tyg'},\n",
    "            {'data': months, 'table': 'miesiace', 'field': 'miesiac'},\n",
    "            {'data': dates, 'table': 'data_czas', 'field': 'data'},\n",
    "            {'data': brands, 'table': 'brandy', 'field': 'brand'},\n",
    "            {'data': lengths, 'table': 'dl_ujednolicone', 'field': 'dl_ujednolicona'},\n",
    "            {'data': dayparts, 'table': 'dayparty', 'field': 'daypart'},\n",
    "            {'data': product_types, 'table': 'typy_produktu', 'field': 'typ_produktu'},\n",
    "            {'data': broadcasters, 'table': 'nadawcy', 'field': 'nadawca'},\n",
    "            {'data': reaches, 'table': 'zasiegi', 'field': 'zasieg'},\n",
    "            ]\n",
    "\n",
    "avoid_adding = ['dni_tyg', 'miesiace', 'dl_ujednolicone', 'dayparty', 'zasiegi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting data to one input tables.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "DataFrame constructor not properly called!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36845/1550168246.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Inserting data into simple tables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Inserting data to one input tables.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0miter_over_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavoid_adding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mexcept\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProgrammingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mcon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Failed to input the data.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\u001b[0m\u001b[0;34mError: \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36845/352401410.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(data_set, con, cur, avoid_adding)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mfield\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'field'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mvar_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_for_data_1_field\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavoid_adding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0madd_1_field\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_36845/352401410.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(data, table_name, field_name, con, cur)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf'\u001b[0m\u001b[0;34mINSERT INTO \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m (\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mfield_name\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m) VALUES(:name);\u001b[0m\u001b[0;34m'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0melem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mto_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'just like that'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36845/352401410.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(elem)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/python/3.12.2/lib/python3.12/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    871\u001b[0m                 \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0;31m# For data is scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DataFrame constructor not properly called!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame constructor not properly called!"
     ]
    }
   ],
   "source": [
    "# Inserting data into simple tables\n",
    "print('Inserting data to one input tables.')\n",
    "try:\n",
    "    iter_over_inputs(data_set, con, cur, avoid_adding)\n",
    "except sqlite3.ProgrammingError as e:\n",
    "    con.close()\n",
    "    print('Failed to input the data.')\n",
    "    print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
