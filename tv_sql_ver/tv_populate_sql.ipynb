{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config.config as cf\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/tv/tv_live_1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {'NEONET.PL': 'NEONET AGD RTV',\n",
    "         'EURO.COM.PL': 'EURO RTV AGD',}\n",
    "dayparts = {'Off stacji': 'off',\n",
    "            'Prime stacji': 'prime'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_val(table_name: str, cur: sqlite3.Cursor)-> int:\n",
    "    \"\"\"\n",
    "    Function gets max index value from the selected table and returns it as an integer increased by one.\n",
    "    When the table is empty, returns 1\n",
    "\n",
    "    :param table_name: Name of the table out of which the data is going to be pulled, \n",
    "    represented as a str\n",
    "    :param cur: Is a cursor object created for con object\n",
    "    :raise sqlite3.ProgrammingError: If there are any error raised by the DB-API\n",
    "    :daise sqlite3.OperationalError: If any eceptions on the DB side are raised, i.g. DB being locked\n",
    "    :return: number representiung max index value of selected table icreased by 1\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    \n",
    "    query = (f'SELECT MAX(id) FROM {table_name};')\n",
    "    cur.execute(query)\n",
    "    ind = cur.fetchone()\n",
    "    if ind[0] == None:\n",
    "        return 1\n",
    "    else:\n",
    "        return ind[0] + 1\n",
    "    \n",
    "def iter_over_inputs(data_set:list[dict[list,str,str]], con: sqlite3.Connection, \n",
    "                        cur: sqlite3.Cursor)-> None:\n",
    "    \"\"\"\n",
    "    Main loop for iteration over one column tables.\n",
    "\n",
    "    :param data_set: List containing dicts with data, table name and field/column name.\n",
    "    List contains strings or integers representing the data to be added into selected tables.\n",
    "    :param table_name: String representing name of the table into which data is going to be added\n",
    "    :param con: Is a connection object, pointing to a DB\n",
    "    :param cur: Is a cursor object created for con object\n",
    "    :param avoid_adding: List of tablet which doesn't need to be updated\n",
    "    :raise KeyError: If key name does not match the pattern\n",
    "    :raise sqlite3.ProgrammingError: If there are any error raised by the DB-API\n",
    "    :daise sqlite3.OperationalError: If any eceptions on the DB side are raised, i.g. DB being locked\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    for elem in data_set:\n",
    "        data = elem['data']\n",
    "        table = elem['table']\n",
    "        field = elem['field']\n",
    "        new_data, data = check_for_data_1_field(data, table, field, cur, avoid_adding)\n",
    "        if new_data:\n",
    "            add_1_field(data, table, field, con, cur)\n",
    "\n",
    "def add_1_field(data:list, table_name:str, field_name: str, \n",
    "                con: sqlite3.Connection, cur: sqlite3.Cursor)-> None:\n",
    "    \"\"\"\n",
    "    Skeleton function for adding data to single column tables.\n",
    "\n",
    "    :param data: List of strings or integers representing table contents\n",
    "    :param table_name: String reprexsenting name of the table into which data is going to be added\n",
    "    :param con: Is a connection object, pointing to a DB\n",
    "    :param cur: Is a cursor object created for con object\n",
    "    :raise sqlite3.ProgrammingError: If there are any error raised by the DB-API\n",
    "    :daise sqlite3.OperationalError: If any eceptions on the DB side are raised, i.g. DB being locked\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    query = (f'INSERT INTO {table_name} ({field_name}) VALUES(:name);')\n",
    "    for elem in data:\n",
    "        to_add = {'name': str(elem)}\n",
    "        cur.execute(query, to_add)\n",
    "    con.commit()\n",
    "\n",
    "def check_for_data_1_field(data_:list, table_name:str, field_name:str, \n",
    "                            cur: sqlite3.Cursor, avoid_adding: list[str])-> tuple[bool,list[str]]:\n",
    "    \"\"\"\n",
    "    Skeleton function for checking if there is data inside each of one column tables.\n",
    "    Ads data if there are any new entries, skips if no new data was found. \n",
    "    If DB is empty returns immediately.\n",
    "\n",
    "    :param data_: List containing data to be checked and added. Data is of str or int types.\n",
    "    :param table_name: String representing name of the table into which data is going to be added\n",
    "    :param field_name: String representing name of the field/ column name\n",
    "    :param cur: Is a cursor object created for con object\n",
    "    :param avoid_adding: List of tablet which doesn't need to be updated\n",
    "    :raise sqlite3.ProgrammingError: If there is an error raised by the DB-API\n",
    "    :daise sqlite3.OperationalError: If any eceptions on the DB side are raised, i.g. DB being locked\n",
    "    :return: A tuple containing bool for logic purposes, anbd the data set to be added\n",
    "    :rtype: tuple[bool, list[str/int]]\n",
    "    \"\"\"\n",
    "    \n",
    "    if table_name == 'kody_rek':\n",
    "        query = (f\"SELECT kod_rek FROM {table_name};\")\n",
    "    else:\n",
    "        query = (f\"SELECT {field_name} FROM {table_name};\")\n",
    "    cur.execute(query)\n",
    "    \n",
    "    in_db = pd.DataFrame([elem[0] for elem in cur.fetchall()])\n",
    "    in_db.rename(columns={0: field_name}, inplace=True)\n",
    "    \n",
    "    if len(in_db) == 0:\n",
    "        return (True, data_)\n",
    "    else:\n",
    "        if field_name == 'data':\n",
    "            in_db = list(in_db['data'])\n",
    "        elif table_name == 'kody_rek':\n",
    "            pass\n",
    "        else: \n",
    "            in_db = list(in_db[field_name])\n",
    "        \n",
    "        if len(in_db) != 0 and table_name in avoid_adding:\n",
    "            print(f'>>> Not adding to {table_name}. No new data found.')\n",
    "            return (False, list(''))\n",
    "        if table_name == 'kody_rek':\n",
    "            in_df = pd.DataFrame(data_)\n",
    "            in_df = in_df[0].str.split(pat='@|@', expand=True, regex=False)\n",
    "        else:\n",
    "            in_df = pd.DataFrame(data_)\n",
    "            in_df = in_df.rename(columns={0: field_name})\n",
    "        \n",
    "        # we check if df contains new data in comparison to DB\n",
    "        if table_name == 'kody_rek':\n",
    "            in_db = in_db.astype('int32')\n",
    "            in_df = in_df.astype({0: 'int32', 1:'object'})\n",
    "            filtr = ~in_df[0].isin(in_db[field_name])\n",
    "            new_data = in_df[filtr]\n",
    "            new_data = new_data.rename({0:'a', 1:'b'}, axis=1)\n",
    "            new_data.loc[:, field_name] = new_data.loc[:, ['a', 'b']].apply(lambda x: f'{x['a']}@|@{x['b']}', axis=1)\n",
    "            new_data.drop(['a', 'b'], axis=1, inplace=True)\n",
    "            new_data.astype({field_name:'object'})\n",
    "            new_data = new_data[field_name]\n",
    "        else:\n",
    "            new_data = in_df[~in_df.isin(in_db)].dropna()\n",
    "            new_data = new_data[field_name]\n",
    "            \n",
    "        new_data = list(new_data)\n",
    "        \n",
    "        if len(new_data) != 0:\n",
    "            print(f'>>> Adding to {table_name}. New data found.')\n",
    "            return (True, new_data)\n",
    "        else:\n",
    "            print(f'>>> Not adding to {table_name}. No new data found.')\n",
    "            return (False, list(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oppening connection.\n"
     ]
    }
   ],
   "source": [
    "# Openes connection to the DB\n",
    "print('Oppening connection.')\n",
    "con = sqlite3.Connection(cf.DB_PATH)\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 972606 entries, 0 to 972605\n",
      "Data columns (total 27 columns):\n",
      " #   Column                                      Non-Null Count   Dtype         \n",
      "---  ------                                      --------------   -----         \n",
      " 0   Month                                       972606 non-null  category      \n",
      " 1   Week                                        972606 non-null  category      \n",
      " 2   Weekday                                     972606 non-null  category      \n",
      " 3   Date                                        972606 non-null  datetime64[ns]\n",
      " 4   Time                                        972606 non-null  object        \n",
      " 5   Dayparts                                    972606 non-null  object        \n",
      " 6   Channel Groups                              972606 non-null  object        \n",
      " 7   Channel                                     972606 non-null  object        \n",
      " 8   PIB pos                                     972606 non-null  int8          \n",
      " 9   PIB (real) rel                              972606 non-null  object        \n",
      " 10  PIB count                                   972606 non-null  int16         \n",
      " 11  Dur rounded,sp                              972606 non-null  int8          \n",
      " 12  Spot Class                                  972606 non-null  object        \n",
      " 13  Block Code                                  972606 non-null  object        \n",
      " 14  Syndicate                                   972606 non-null  object        \n",
      " 15  Producer                                    972606 non-null  object        \n",
      " 16  Brand                                       972606 non-null  object        \n",
      " 17  Film Code                                   972606 non-null  object        \n",
      " 18  Prog Campaign                               972606 non-null  object        \n",
      " 19  Prog Before                                 972606 non-null  object        \n",
      " 20  Prog After                                  972606 non-null  object        \n",
      " 21  Film Code/2                                 972606 non-null  object        \n",
      " 22  TRP All 40+ cities 200- (i wsie) Nat[TSV2]  972606 non-null  float64       \n",
      " 23  DateTime                                    972606 non-null  datetime64[ns]\n",
      " 24  Kod Opis                                    972606 non-null  object        \n",
      " 25  Kanal Grupa                                 972606 non-null  object        \n",
      " 26  Brand Prod Synd                             972606 non-null  object        \n",
      "dtypes: category(3), datetime64[ns](2), float64(1), int16(1), int8(2), object(18)\n",
      "memory usage: 162.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_tv = pd.read_csv(file_path, delimiter=';', thousands=' ', decimal=',', dtype={\n",
    "                    'Month': 'category', 'Week': 'category', 'Weekday': 'category',\n",
    "                    'Dayparts': 'object', 'Channel Groups': 'object', \n",
    "                    'Channel': 'object', 'PIB pos': 'int8', 'PIB (real) rel': 'object',\n",
    "                    'PIB count': 'int16', 'Dur rounded,sp': 'int8', 'Spot Class': 'object',\n",
    "                    'Block Code': 'object', 'Syndicate': 'object', 'Producer': 'object',\n",
    "                    'Brand': 'object', 'Film Code': 'object', 'Prog Campaign': 'object',\n",
    "                    'Prog Before': 'object', 'Prog After': 'object', \n",
    "                    'Film Code/2': 'object'}, \n",
    "                    parse_dates=['Date', 'Time'], date_format='%d.%m.%Y'\n",
    "                    )\n",
    "df_tv['Brand'] = df_tv['Brand'].str.strip()\n",
    "df_tv['Brand'] = df_tv['Brand'].str.upper()\n",
    "df_tv['Producer'] = df_tv['Producer'].str.upper()\n",
    "df_tv['Syndicate'] = df_tv['Syndicate'].str.upper()\n",
    "df_tv['Brand'] = df_tv['Brand'].map(names).fillna(df_tv['Brand'])\n",
    "df_tv['Dayparts'] = df_tv['Dayparts'].map(dayparts).fillna(df_tv['Dayparts'])\n",
    "df_tv['DateTime'] = df_tv['Date'].astype('str') + ' ' + df_tv['Time']\n",
    "df_tv['Kod Opis'] = df_tv['Film Code/2'] + '@|@' + df_tv['Film Code']\n",
    "df_tv['Kanal Grupa'] = df_tv['Channel'] + '@|@' + df_tv['Channel Groups']\n",
    "df_tv['Brand Prod Synd'] = df_tv['Brand'] + '@|@' + df_tv['Producer'] + '#|#' + df_tv['Syndicate']\n",
    "df_tv['DateTime'] = pd.to_datetime(df_tv['DateTime'], format='ISO8601')\n",
    "df_tv[['Dayparts', 'Prog Before', 'Prog After']] = df_tv[['Dayparts', 'Prog Before', 'Prog After']].fillna('brak danych', axis=1)\n",
    "df_tv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df_tv['Date'].dt.strftime('%Y-%m-%d').unique()\n",
    "ad_codes = df_tv.loc[:, ['Film Code/2', 'Film Code', 'Kod Opis']].drop_duplicates(subset=['Film Code/2'], keep='first', ignore_index=True)\n",
    "ad_codes = ad_codes['Kod Opis']\n",
    "brands = df_tv.loc[:, ['Brand', 'Producer', 'Syndicate', 'Brand Prod Synd']].drop_duplicates(subset=['Brand'], keep='first', ignore_index=True)\n",
    "brands = brands['Brand Prod Synd']\n",
    "channels = df_tv.loc[:, ['Channel', 'Channel Groups', 'Kanal Grupa']].drop_duplicates(subset=['Channel'], keep='first', ignore_index=True)\n",
    "channels = channels['Kanal Grupa']\n",
    "dayparts = df_tv['Dayparts'].unique()\n",
    "pib_real_rels = df_tv['PIB (real) rel'].unique()\n",
    "durations = df_tv['Dur rounded,sp'].unique()\n",
    "spot_classes = df_tv['Spot Class'].unique()\n",
    "block_codes = df_tv['Block Code'].unique()\n",
    "prog_campaign = df_tv['Prog Campaign'].unique()\n",
    "prog_before= df_tv['Prog Before'].unique()\n",
    "prog_after = df_tv['Prog After'].unique()\n",
    "\n",
    "data_set = [{'data': dates, 'table': cf.DATES['table'], 'field': cf.DATES['field'], 'type': cf.DATES['type']},\n",
    "            {'data': ad_codes, 'table': cf.AD_CODE['table'], 'field': cf.AD_CODE['field'], 'type': cf.AD_CODE['type']},\n",
    "            {'data': brands, 'table': cf.BRANDS['table'], 'field': cf.BRANDS['field'], 'type': cf.BRANDS['type']},\n",
    "            {'data': channels, 'table': cf.CHANNELS['table'], 'field': cf.CHANNELS['field'], 'type': cf.CHANNELS['type']},\n",
    "            {'data': dayparts, 'table': cf.DAYPARTS['table'], 'field': cf.DAYPARTS['field'], 'type': cf.DAYPARTS['type']},\n",
    "            {'data': pib_real_rels, 'table': cf.PIB_R['table'], 'field': cf.PIB_R['field'], 'type': cf.PIB_R['type']},\n",
    "            {'data': durations, 'table': cf.DUR['table'], 'field': cf.DUR['field'], 'type': cf.DUR['type']},\n",
    "            {'data': spot_classes, 'table': cf.SPOT_CLASS['table'], 'field': cf.SPOT_CLASS['field'], 'type': cf.SPOT_CLASS['type']},\n",
    "            {'data': block_codes, 'table': cf.BLOCK_CODE['table'], 'field': cf.BLOCK_CODE['field'], 'type': cf.BLOCK_CODE['type']},\n",
    "            {'data': prog_campaign, 'table': cf.PR_CAMP['table'], 'field': cf.PR_CAMP['field'], 'type': cf.PR_CAMP['type']},\n",
    "            {'data': prog_before, 'table': cf.PR_BEF['table'], 'field': cf.PR_BEF['field'], 'type': cf.PR_BEF['type']},\n",
    "            {'data': prog_after, 'table': cf.PR_AFT['table'], 'field': cf.PR_AFT['field'], 'type': cf.PR_AFT['type']},\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    iter_over_inputs(data_set, con, cur)\n",
    "except sqlite3.ProgrammingError as e:\n",
    "    con.close()\n",
    "    print('Failed to input the data.')\n",
    "    print(f'Error: {e}')\n",
    "except sqlite3.OperationalError as e:\n",
    "    con.close()\n",
    "    print('Failed to input the data.')\n",
    "    print(f'Error: {e}')\n",
    "    exit()\n",
    "    \n",
    "# TODO CHANGE iter_over_inputs table for new key handling, and create logic for tables populated by triggers!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
