{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools.conf\n",
    "import psycopg\n",
    "from psycopg import sql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_1_field(data:list, table_name:str, field_name: str)-> None:\n",
    "    \"\"\"\n",
    "    Skeleton function for adding data to single column tables.\n",
    "\n",
    "    :param data: List of strings or integers representing table contents\n",
    "    :param table_name: String reprexsenting name of the table into which data is going to be added\n",
    "    :raise psycopg.DataError: If data type does not match table restrictions\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    query = sql.SQL('INSERT INTO {table} ({field}) VALUES (%s)')\n",
    "\n",
    "    for elem in data:\n",
    "        cur.execute(\n",
    "            query.format(\n",
    "                table=sql.Identifier(f'{table_name}'),\n",
    "                field=sql.Identifier(f'{field_name}')), (elem,)\n",
    "        )\n",
    "    conn.commit()\n",
    "\n",
    "def iter_over_inputs(data_set:list[dict[list,str,str]])-> None:\n",
    "    \"\"\"\n",
    "    Main loop for iteration over one column tables.\n",
    "\n",
    "    :param data_set: List containing dicts with data, table name and field/column name.\n",
    "    List contains strings or integers representing the data to be added into selected tables.\n",
    "    :param table_name: String representing name of the table into which data is going to be added\n",
    "    :raise KeyError: If key name does not match the pattern\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    for elem in data_set:\n",
    "        data = elem['data']\n",
    "        table = elem['table']\n",
    "        field = elem['field']\n",
    "        new_data, data = check_for_data_1_field(data, table, field)\n",
    "        if new_data:\n",
    "            add_1_field(data, table, field)\n",
    "\n",
    "def check_for_data_1_field(data_:list, table_name:str, field_name:str)-> tuple[bool,list[str]]:\n",
    "    \"\"\"\n",
    "    Skeleton function for checking if there is data inside each of one column tables.\n",
    "    Ads data if there are any new entries, skips if no new data was found. \n",
    "    If DB is empty returns immediately.\n",
    "\n",
    "    :param data_: List containing data to be checked and added. Data is of str or int types.\n",
    "    :param table_name: String representing name of the table into which data is going to be added\n",
    "    :param field_name: String representing name of the field/ column name\n",
    "    :raise psycopg.DataError: If data type does not match table restrictions\n",
    "    :return: A tuple containing bool for logic purposes, anbd the data set to be added\n",
    "    :rtype: tuple[bool, list[str/int]]\n",
    "    \"\"\"\n",
    "    \n",
    "    query = sql.SQL('SELECT {field} FROM {table}')\n",
    "    cur.execute(\n",
    "        query.format(\n",
    "            table=sql.Identifier(table_name),\n",
    "            field=sql.Identifier(field_name))\n",
    "    )\n",
    "    \n",
    "    in_db = pd.DataFrame([elem[0] for elem in cur.fetchall()])\n",
    "    in_db.rename(columns={0: field_name}, inplace=True)\n",
    "    \n",
    "    if len(in_db) == 0:\n",
    "        return (True, data_)\n",
    "    else:\n",
    "        if field_name == 'date':\n",
    "            in_db['date'] = pd.to_datetime(in_db['date'])\n",
    "            in_db = list(in_db['date'])\n",
    "        else: \n",
    "            in_db = list(in_db[field_name])\n",
    "        \n",
    "        if len(in_db) != 0 and table_name in avoid_adding:\n",
    "            print(f'>>> Not adding to {table_name}. No new data found.')\n",
    "            return (False, list(''))\n",
    "        in_df = pd.DataFrame(data_)\n",
    "        in_df = in_df.rename(columns={0: field_name})\n",
    "        if field_name == 'date':\n",
    "            in_df['date'] = pd.to_datetime(in_df['date'])\n",
    "        \n",
    "        # we check if df contains new data in comparison to DB\n",
    "        new_data = in_df[~in_df.isin(in_db)].dropna()\n",
    "        new_data = new_data[field_name]\n",
    "        new_data = list(new_data)\n",
    "        \n",
    "        if len(new_data) != 0:\n",
    "            print(f'>>> Adding to {table_name}. New data found.')\n",
    "            return (True, new_data)\n",
    "        else:\n",
    "            print(f'>>> Not adding to {table_name}. No new data found.')\n",
    "            return (False, list(''))\n",
    "\n",
    "def add_3_fields(data_set:dict[pd.DataFrame,str,list])-> None:\n",
    "    \"\"\"\n",
    "    Function adding data into mediums table, which consists of 3 columns.\n",
    "\n",
    "    :param data_set: A dict contaning data to be added, table name, and field / column name.\n",
    "    Data is a Pandas DataFrame, table name and field name are both strings.\n",
    "    :raise KeyError: If key name does not match the pattern\n",
    "    :raise psycopg.DataError: If table or field names doesn't match those in the DB\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    col = data_set['data'].columns.values.tolist()\n",
    "    \n",
    "    query = sql.SQL('INSERT INTO {table} ({field1}, {field2}, {field3}) VALUES (%s, %s ,%s)')\n",
    "\n",
    "    for _, elem in data_set['data'].iterrows():\n",
    "        cur.execute(\n",
    "            query.format(\n",
    "                table=sql.Identifier(data_set['table']),\n",
    "                field1=sql.Identifier(data_set['fields'][0]),\n",
    "                field2=sql.Identifier(data_set['fields'][1]),\n",
    "                field3=sql.Identifier(data_set['fields'][2])), \n",
    "                (elem[col[0]], elem[col[1]], elem[col[2]],)\n",
    "        )\n",
    "        \n",
    "    conn.commit()\n",
    "    \n",
    "def check_for_data_3_fields(fields:list[str], table_name: str, submediums: pd.DataFrame)-> tuple[bool,pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns a bool for logic purposes and data to be added into mediums table.\n",
    "    If DB is empty returns original DF. During data update process returns the data not present in the DB\n",
    "    or indicates there is nothing to be added.\n",
    "\n",
    "    :param fields: A list containing field / column names represented as a str\n",
    "    :param table_name: Name of the table into which data is going to be added as a str\n",
    "    :param submediums: Pandas DataFrame containing data to add.\n",
    "    :raise psycopg.DataError: If data type does not match table restrictions\n",
    "    :return: Tuple containing bool for logic purposes and a Pandas DataFrame \n",
    "    as data to be added into the DB during the update\n",
    "    :rtype: tuple[bool, pd.DataFrame]\n",
    "    \"\"\"\n",
    "    \n",
    "    query = sql.SQL('SELECT id, {field1}, {field2}, {field3} FROM {table}')\n",
    "    cur.execute(\n",
    "        query.format(\n",
    "            table=sql.Identifier(table_name),\n",
    "            field1=sql.Identifier(fields[0]),\n",
    "            field2=sql.Identifier(fields[1]),\n",
    "            field3=sql.Identifier(fields[2]))\n",
    "    )\n",
    "    \n",
    "    in_db = pd.DataFrame(cur.fetchall())\n",
    "    in_db.rename(columns={0: 'id',1: fields[0], 2: fields[1], 3: fields[2]}, inplace=True)\n",
    "    \n",
    "    if len(in_db) == 0:\n",
    "        return (True, submediums)\n",
    "    else:\n",
    "        in_db = list(in_db[fields[0]])\n",
    "        in_df = submediums.copy()\n",
    "        # we check if df contains new data in comparison to DB\n",
    "        new_data = in_df[~in_df.isin(in_db)].dropna()\n",
    "        \n",
    "        if len(new_data) != 0 :\n",
    "            print(f'>>> Adding to {table_name}. New data found.')\n",
    "            return (True, new_data)\n",
    "        print(f'>>> Not adding to {table_name}. No new data found.')\n",
    "        return (False, submediums)\n",
    "\n",
    "def add_8_fields(data_set:dict[pd.DataFrame,str,list[str]])-> None:\n",
    "    \"\"\"\n",
    "    Function adding data into ad_time_details table, which consists of 8 columns.\n",
    "\n",
    "    :param data_set: A dict contaning data to be added, table name, and field / column name.\n",
    "    Data is a Pandas DataFrame, table name and field name are both strings.\n",
    "    :raise KeyError: If key name does not match the pattern\n",
    "    :raise psycopg.DataError: If table or field names doesn't match those in the DB\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    col = data_set['data'].columns.values.tolist()\n",
    "    \n",
    "    query = sql.SQL(\n",
    "        '''\n",
    "        INSERT INTO {table} ({field1}, {field2}, {field3}, {field4}, {field5}, {field6}, {field7}, {field8}) \n",
    "        VALUES (%s, %s ,%s, %s ,%s, %s ,%s ,%s)\n",
    "        ''')\n",
    "\n",
    "    for _, elem in data_set['data'].iterrows():\n",
    "        cur.execute(\n",
    "            query.format(\n",
    "                table=sql.Identifier(data_set['table']),\n",
    "                field1=sql.Identifier(data_set['fields'][0]),\n",
    "                field2=sql.Identifier(data_set['fields'][1]),\n",
    "                field3=sql.Identifier(data_set['fields'][2]),\n",
    "                field4=sql.Identifier(data_set['fields'][3]),\n",
    "                field5=sql.Identifier(data_set['fields'][4]),\n",
    "                field6=sql.Identifier(data_set['fields'][5]),\n",
    "                field7=sql.Identifier(data_set['fields'][6]),\n",
    "                field8=sql.Identifier(data_set['fields'][7])), \n",
    "                (elem[col[0]], elem[col[1]], elem[col[2]], \n",
    "                 elem[col[3]], elem[col[4]], elem[col[5]], \n",
    "                 elem[col[6]], elem[col[7]],\n",
    "                )\n",
    "        )\n",
    "        \n",
    "    conn.commit()\n",
    "\n",
    "def add_10_fields(data_set:dict[pd.DataFrame,str,list[str]])-> None:\n",
    "    \"\"\"\n",
    "    Function adding data into ad_time_details table, which consists of 10 columns.\n",
    "\n",
    "    :param data_set: A dict contaning data to be added, table name, and field / column name.\n",
    "    Data is a Pandas DataFrame, table name and field name are both strings.\n",
    "    :raise KeyError: If key name does not match the pattern\n",
    "    :raise psycopg.DataError: If table or field names doesn't match those in the DB\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    col = data_set['data'].columns.values.tolist()\n",
    "    \n",
    "    query = sql.SQL(\n",
    "        '''\n",
    "        INSERT INTO {table} ({field1}, {field2}, {field3}, {field4}, {field5}, {field6}, {field7}, {field8}, {field9}, {field10}) \n",
    "        VALUES (%s, %s ,%s, %s ,%s, %s ,%s ,%s ,%s ,%s)\n",
    "        ''')\n",
    "\n",
    "    for _, elem in data_set['data'].iterrows():\n",
    "        cur.execute(\n",
    "            query.format(\n",
    "                table=sql.Identifier(data_set['table']),\n",
    "                field1=sql.Identifier(data_set['fields'][0]),\n",
    "                field2=sql.Identifier(data_set['fields'][1]),\n",
    "                field3=sql.Identifier(data_set['fields'][2]),\n",
    "                field4=sql.Identifier(data_set['fields'][3]),\n",
    "                field5=sql.Identifier(data_set['fields'][4]),\n",
    "                field6=sql.Identifier(data_set['fields'][5]),\n",
    "                field7=sql.Identifier(data_set['fields'][6]),\n",
    "                field8=sql.Identifier(data_set['fields'][7]),\n",
    "                field9=sql.Identifier(data_set['fields'][8]),\n",
    "                field10=sql.Identifier(data_set['fields'][9])), \n",
    "                (elem[col[0]], elem[col[1]], elem[col[2]], \n",
    "                 elem[col[3]], elem[col[4]], elem[col[5]], \n",
    "                 elem[col[6]], elem[col[7]], elem[col[8]], \n",
    "                 elem[col[9]],\n",
    "                )\n",
    "        )\n",
    "        \n",
    "    conn.commit()\n",
    "\n",
    "def get_id_for_submediums(fields:list[str], table_:str)-> tuple[bool, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Gets IDs from reference tables to mediums table. \n",
    "    Mainly connects submediums with broadcaster and reach tables.\n",
    "    Returns a bool for logic purposes and data to be added into mediums.\n",
    "\n",
    "    :param fields: A list containing field / column names represented as a str\n",
    "    :param table_: Name of the table out of which the data is going to be pulled, \n",
    "    represented as a str\n",
    "    :raise psycopg.DataError: If data type does not match table restrictions\n",
    "    :return: Tuple containing bool for logic purposes and a Pandas DataFrame \n",
    "    as data to be added into the DB during the update or initial DB fill.\n",
    "    :rtype: tuple[bool, pd.DataFrame]\n",
    "    \"\"\"\n",
    "    \n",
    "    submediums = df[['submedium', 'wydawca_nadawca', 'zasięg medium']].sort_values(by='submedium')\n",
    "    submediums.drop_duplicates(subset=['submedium'], keep='first', inplace=True, ignore_index=True)\n",
    "    \n",
    "    if sum(submediums.value_counts()) != submediums.index.max() + 1:\n",
    "        exit('Max index different than the length of the list.')\n",
    "    \n",
    "    query1 = sql.SQL('SELECT {fields} FROM {table}').format(\n",
    "    fields=sql.SQL(',').join([\n",
    "        sql.Identifier('broadcaster'),\n",
    "        sql.Identifier('id')\n",
    "    ]),\n",
    "    table=sql.Identifier('broadcasters'))\n",
    "    cur.execute(query1)\n",
    "    broadcasters = dict(cur.fetchall())\n",
    "\n",
    "    query2 = sql.SQL('SELECT {fields} FROM {table}').format(\n",
    "        fields=sql.SQL(',').join([\n",
    "            sql.Identifier('reach'),\n",
    "            sql.Identifier('id')\n",
    "        ]),\n",
    "        table=sql.Identifier('ad_reach'))\n",
    "    cur.execute(query2)\n",
    "    ad_reach = dict(cur.fetchall())\n",
    "\n",
    "    submediums['wydawca_nadawca'] = submediums['wydawca_nadawca'].map(broadcasters)\n",
    "    submediums['zasięg medium'] = submediums['zasięg medium'].map(ad_reach)\n",
    "    \n",
    "    trigger, submediums = check_for_data_3_fields(fields, table_, submediums)\n",
    "    \n",
    "    return (trigger, submediums)\n",
    "\n",
    "def get_id_for_ad_time(fields: list[str], table_: str)-> tuple[bool,pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Gets IDs from reference tables to ad_time_details table. \n",
    "    Mainly connects time details of singular ad emission with other tables containing details via IDs.\n",
    "    This function populates one of two core tables in this DB.\n",
    "    Returns a bool for logic purposes and data to be added into mediums.\n",
    "\n",
    "    :param fields: A list containing field / column names represented as a str\n",
    "    :param table_: Name of the table out of which the data is going to be pulled, \n",
    "    represented as a str\n",
    "    :raise psycopg.DataError: If data type does not match table restrictions\n",
    "    :return: Tuple containing bool for logic purposes and a Pandas DataFrame \n",
    "    as data to be added into the DB during the update or initial DB fill.\n",
    "    :rtype: tuple[bool, pd.DataFrame]\n",
    "    \"\"\"\n",
    "    \n",
    "    ad_time = df[['data', 'godzina_bloku_reklamowego', 'gg', 'mm', 'dl_mod', 'daypart', 'dł_ujednolicona', 'ad_time_details']]\n",
    "    ad_time.index = ad_time.index + get_index_val(table_)\n",
    "\n",
    "    query1 = sql.SQL('SELECT {fields} FROM {table}').format(\n",
    "        fields=sql.SQL(',').join([\n",
    "            sql.Identifier('length'),\n",
    "            sql.Identifier('id')\n",
    "            ]),\n",
    "        table=sql.Identifier('unified_lengths'))\n",
    "\n",
    "    cur.execute(query1)\n",
    "    unified_lengths = dict(cur.fetchall())\n",
    "\n",
    "    query2 = sql.SQL('SELECT {fields} FROM {table}').format(\n",
    "        fields=sql.SQL(',').join([\n",
    "            sql.Identifier('daypart'),\n",
    "            sql.Identifier('id')\n",
    "            ]),\n",
    "        table=sql.Identifier('dayparts'))\n",
    "\n",
    "    cur.execute(query2)\n",
    "    dayparts = dict(cur.fetchall())\n",
    "\n",
    "    ad_time.loc[:, 'daypart'] = ad_time['daypart'].map(dayparts)\n",
    "    ad_time.loc[:, 'dł_ujednolicona'] = ad_time['dł_ujednolicona'].map(unified_lengths)\n",
    "    \n",
    "    trigger, ad_time = get_min_max_date(fields, table_, ad_time)\n",
    "    \n",
    "    return (trigger, ad_time)\n",
    "\n",
    "def get_id_for_ads_desc(fields: list[str], table_: str)-> tuple[bool,pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Gets IDs from reference tables to ads_desc table. \n",
    "    Mainly connects other tables and data of singular ad emission via IDs with other tables.\n",
    "    This function populates one of two core tables in this DB.\n",
    "    Returns a bool for logic purposes and data to be added into mediums.\n",
    "\n",
    "    :param fields: A list containing field / column names represented as a str\n",
    "    :param table_: Name of the table out of which the data is going to be pulled, \n",
    "    represented as a str\n",
    "    :raise psycopg.DataError: If data type does not match table restrictions\n",
    "    :return: Tuple containing bool for logic purposes and a Pandas DataFrame \n",
    "    as data to be added into the DB during the update or initial DB fill.\n",
    "    :rtype: tuple[bool, pd.DataFrame]\n",
    "    \"\"\"\n",
    "    \n",
    "    ads_desc = df[['data', 'opis_reklamy', 'kod_reklamy', 'brand', 'submedium', 'ad_time_details', 'produkt(4)', 'koszt', 'l_emisji', 'typ_reklamy']]\n",
    "    ads_desc.index = ads_desc.index + get_index_val(table_)\n",
    "\n",
    "    query1 = sql.SQL('SELECT {fields} FROM {table}').format(\n",
    "        fields=sql.SQL(',').join([\n",
    "            sql.Identifier('brand'),\n",
    "            sql.Identifier('id')\n",
    "            ]),\n",
    "        table=sql.Identifier('brands'))\n",
    "\n",
    "    cur.execute(query1)\n",
    "    brands_id = dict(cur.fetchall())\n",
    "\n",
    "\n",
    "    query2 = sql.SQL('SELECT {fields} FROM {table}').format(\n",
    "        fields=sql.SQL(',').join([\n",
    "            sql.Identifier('submedium'),\n",
    "            sql.Identifier('id')\n",
    "            ]),\n",
    "        table=sql.Identifier('mediums'))\n",
    "\n",
    "    cur.execute(query2)\n",
    "    medium_id = dict(cur.fetchall())\n",
    "\n",
    "\n",
    "    query3 = sql.SQL('SELECT {fields} FROM {table}').format(\n",
    "        fields=sql.SQL(',').join([\n",
    "            sql.Identifier('ad_code'),\n",
    "            sql.Identifier('id')\n",
    "            ]),\n",
    "        table=sql.Identifier('ad_time_details'))\n",
    "\n",
    "    cur.execute(query3)\n",
    "    ad_time_details_id = dict(cur.fetchall())\n",
    "\n",
    "\n",
    "    query4 = sql.SQL('SELECT {fields} FROM {table}').format(\n",
    "        fields=sql.SQL(',').join([\n",
    "            sql.Identifier('product_type'),\n",
    "            sql.Identifier('id')\n",
    "            ]),\n",
    "        table=sql.Identifier('product_types'))\n",
    "\n",
    "    cur.execute(query4)\n",
    "    product_type_id = dict(cur.fetchall())\n",
    "\n",
    "    ads_desc.loc[:, 'brand'] = ads_desc['brand'].map(brands_id)\n",
    "    ads_desc.loc[:, 'submedium'] = ads_desc['submedium'].map(medium_id)\n",
    "    ads_desc.loc[:, 'ad_time_details'] = ads_desc['ad_time_details'].map(ad_time_details_id)\n",
    "    ads_desc.loc[:, 'produkt(4)'] = ads_desc['produkt(4)'].map(product_type_id)\n",
    "    \n",
    "    trigger, ads_desc = get_min_max_date(fields, table_, ads_desc)\n",
    "    \n",
    "    return (trigger, ads_desc)\n",
    "\n",
    "def get_colum_names(table_name:str)->list[str]:\n",
    "    \"\"\"\n",
    "    A function which returns the names of selected table from the DB.\n",
    "\n",
    "    :raise psycopg.DatabaseError: If column names does not match DB contents\n",
    "    :return: List containing all the column names present in selected table. \n",
    "    :rtype: list[str]\n",
    "    \"\"\"\n",
    "    \n",
    "    query = sql.SQL(\n",
    "    '''\n",
    "    SELECT c.column_name \n",
    "    FROM information_schema.columns c \n",
    "    WHERE c.table_name = %s\n",
    "    ORDER BY c.ordinal_position;\n",
    "    ''').format()\n",
    "    cur.execute(query, (table_name,))\n",
    "    table_data = cur.fetchall()\n",
    "    temp = []\n",
    "    for elem in table_data[1:]:\n",
    "        temp.append(elem[0])\n",
    "    table_data = temp\n",
    "    \n",
    "    return table_data\n",
    "\n",
    "def get_index_val(table_name: str)-> int:\n",
    "    \"\"\"\n",
    "    Function gets max index value from the selected table and returns it as an integer increased by one.\n",
    "    When the table is empty, returns 1\n",
    "\n",
    "    :param table_name: Name of the table out of which the data is going to be pulled, \n",
    "    represented as a str\n",
    "    :raise psycopg.DataError: If data type does not match table restrictions\n",
    "    :return: number representiung max index value of selected table icreased by 1\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    \n",
    "    query = sql.SQL('SELECT MAX(id) FROM {table};')\n",
    "    cur.execute(query.format(table=sql.Identifier(table_name)))\n",
    "    ind = cur.fetchone()\n",
    "    if ind[0] == None:\n",
    "        return 1\n",
    "    else:\n",
    "        return ind[0] + 1\n",
    "\n",
    "def get_min_max_date(fields: list[str], table_: str, dataframe: pd.DataFrame)-> tuple[bool, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Gets max and min dates from selected table. Then checks if dates present in the DF passed as a parameter\n",
    "    are outside of dates range. If so, allows data insertion into the DB, if not it informs the user, \n",
    "    and proceedes with the rest of the code.\n",
    "\n",
    "    :param fields: A list containing field / column names represented as a str\n",
    "    :param table_: Name of the table out of which the data is going to be pulled, \n",
    "    represented as a str\n",
    "    :param dataframe: Pandas DataFrame with the new data to be checked if not present in selected table\n",
    "    :raise psycopg.DatabaseError: If column names does not match DB contents\n",
    "    :return: Tuple containing bool for logic purposes and a Pandas DataFrame \n",
    "    as data to be added into the DB during the update or initial DB fill.\n",
    "    :rtype: tuple[bool, pd.DataFrame]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get max date from DB\n",
    "    query = sql.SQL('SELECT MAX({field1}) FROM {table};')\n",
    "    cur.execute(\n",
    "        query.format(\n",
    "            table=sql.Identifier(table_),\n",
    "            field1=sql.Identifier(fields[0])\n",
    "        )\n",
    "    )\n",
    "    in_db_max = pd.Timestamp(cur.fetchone()[0])\n",
    "    \n",
    "    # Get min date from DB\n",
    "    query = sql.SQL('SELECT MIN({field1}) FROM {table};')\n",
    "    cur.execute(\n",
    "        query.format(\n",
    "            table=sql.Identifier(table_),\n",
    "            field1=sql.Identifier(fields[0])\n",
    "        )\n",
    "    )\n",
    "    in_db_min = pd.Timestamp(cur.fetchone()[0])\n",
    "    \n",
    "    # Get max and min date from DF\n",
    "    in_df_max = dataframe['data'].max()\n",
    "    in_df_min = dataframe['data'].min()\n",
    "    \n",
    "    # Check if min and max dates from DF are between range of dates from DB\n",
    "    min_df_in_db_range = in_db_min <= in_df_min <= in_db_max\n",
    "    max_df_in_db_range = in_db_min <= in_df_max <= in_db_max\n",
    "    \n",
    "    # Main logic add if empty or when dates not present in DB.\n",
    "    if  pd.isnull(in_db_max) or pd.isnull(in_db_min) :\n",
    "        return (True, dataframe)\n",
    "    elif not min_df_in_db_range and not max_df_in_db_range:\n",
    "        print(f'>>> Adding to {table_}. New data found.')\n",
    "        return (True, dataframe)\n",
    "    else:\n",
    "        print(f'>>> Not adding to {table_}. One or more dates already in DB.')\n",
    "        print(f'>>> Check the data you want to insert into DB.')\n",
    "        return (False, dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16196/4265642212.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  df['ad_time_details'] = df[['data', 'kod_reklamy']].apply(lambda x: f'{x[0]} - {x[1]} - {ind[x.name]}', axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Openes connection to the DB\n",
    "print('Oppening connection.')\n",
    "conn = psycopg.connect(\n",
    "    f'''dbname={tools.conf.DB}\n",
    "        user={tools.conf.USER}\n",
    "        host={tools.conf.HOST}\n",
    "        port={tools.conf.PORT}\n",
    "    '''\n",
    ")\n",
    "\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating DataFrame.')\n",
    "# Reads the dataframe\n",
    "df = pd.read_csv('./data/baza2.csv', delimiter=';', thousands=',', dtype={'dł_ujednolicona': 'object'}, encoding='utf-8', parse_dates=['data'])\n",
    "df.sort_values(by='data', axis=0, inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "df.drop('index', axis=1, inplace=True)\n",
    "ind = df.index.values + get_index_val('ads_desc')\n",
    "df['ad_time_details'] = df[['data', 'kod_reklamy']].apply(lambda x: f'{x[\"data\"]} - {x[\"kod_reklamy\"]} - {ind[x.name]}', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets for simple tables\n",
    "dow2 = ['Poniedziałek', 'Wtorek', 'Środa', 'Czwartek', 'Piątek',\n",
    "        'Sobota', 'Niedziela']\n",
    "months = [\n",
    "    'Styczeń', 'Luty', 'Marzec', 'Kwiecień', 'Maj',\n",
    "    'Czerwiec', 'Lipiec', 'Sierpień', 'Wrzesień',\n",
    "    'Październik', 'Listopad', 'Grudzień'\n",
    "]\n",
    "dates = df['data'].unique()\n",
    "brands = df['brand'].sort_values().unique()\n",
    "lengths = df['dł_ujednolicona'].sort_values().unique()\n",
    "dayparts = df['daypart'].unique()\n",
    "product_types = df['produkt(4)'].sort_values().unique()\n",
    "broadcasters = df['wydawca_nadawca'].sort_values().unique()\n",
    "reaches = df['zasięg medium'].unique()\n",
    "\n",
    "data_set = [{'data': dow2, 'table': 'pl_dow_names', 'field': 'dow_name'},\n",
    "            {'data': months, 'table': 'pl_month_names', 'field': 'month_name'},\n",
    "            {'data': dates, 'table': 'date_time', 'field': 'date'},\n",
    "            {'data': brands, 'table': 'brands', 'field': 'brand'},\n",
    "            {'data': lengths, 'table': 'unified_lengths', 'field': 'length'},\n",
    "            {'data': dayparts, 'table': 'dayparts', 'field': 'daypart'},\n",
    "            {'data': product_types, 'table': 'product_types', 'field': 'product_type'},\n",
    "            {'data': broadcasters, 'table': 'broadcasters', 'field': 'broadcaster'},\n",
    "            {'data': reaches, 'table': 'ad_reach', 'field': 'reach'},\n",
    "            ]\n",
    "\n",
    "avoid_adding = ['pl_dow_names', 'pl_month_names', 'dayparts', 'ad_reach']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16196/222699361.py:213: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ads_desc['brand'] = ads_desc['brand'].map(brands_id)\n",
      "/tmp/ipykernel_16196/222699361.py:214: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ads_desc['submedium'] = ads_desc['submedium'].map(medium_id)\n",
      "/tmp/ipykernel_16196/222699361.py:215: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ads_desc['ad_time_details'] = ads_desc['ad_time_details'].map(ad_time_details_id)\n",
      "/tmp/ipykernel_16196/222699361.py:216: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ads_desc['produkt(4)'] = ads_desc['produkt(4)'].map(product_type_id)\n"
     ]
    }
   ],
   "source": [
    "# Inserting data into simple tables\n",
    "print('Inserting data to one input tables.')\n",
    "try:\n",
    "    iter_over_inputs(data_set)\n",
    "except psycopg.OperationalError as e:\n",
    "    conn.close()\n",
    "    print('Failed to input the data.')\n",
    "    print(f'Error: {e}')\n",
    "\n",
    "# Create and insert data into mediums table\n",
    "print('Inserting data to the three input table.')\n",
    "fields = get_colum_names('mediums')\n",
    "trigger, submediums = get_id_for_submediums(fields, 'mediums')\n",
    "data_set2 = {'data': submediums, 'table': 'mediums', 'fields': fields}\n",
    "if trigger:\n",
    "    try:\n",
    "        add_3_fields(data_set2)\n",
    "    except psycopg.OperationalError as e:\n",
    "        conn.close()\n",
    "        print('Failed to input the data.')\n",
    "        print(f'Error: {e}')\n",
    "\n",
    "# Create and insert data into ad_time_details table\n",
    "print('Inserting data to the eight input table.')\n",
    "fields = get_colum_names('ad_time_details')\n",
    "trigger, ad_time = get_id_for_ad_time(fields, 'ad_time_details')\n",
    "data_set3 = {'data': ad_time, 'table': 'ad_time_details', 'fields': fields}\n",
    "if trigger:\n",
    "    try:\n",
    "        add_8_fields(data_set3)\n",
    "    except psycopg.OperationalError as e:\n",
    "        conn.close()\n",
    "        print('Failed to input the data.')\n",
    "        print(f'Error: {e}')\n",
    "\n",
    "# Create and insert data into ad_time_details table\n",
    "print('Inserting data to the ten input table.')\n",
    "fields = get_colum_names('ads_desc')\n",
    "trigger, ads_desc = get_id_for_ads_desc(fields, 'ads_desc')\n",
    "data_set4 = {'data': ads_desc, 'table': 'ads_desc', 'fields': fields}\n",
    "if trigger:\n",
    "    try:\n",
    "        add_10_fields(data_set4)\n",
    "    except psycopg.OperationalError as e:\n",
    "        conn.close()\n",
    "        print('Failed to input the data.')\n",
    "        print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
